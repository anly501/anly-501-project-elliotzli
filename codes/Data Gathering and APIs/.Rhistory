library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(twitteR)
library(ROAuth)
library(jsonlite)
consumerKey=as.character('JKjzpgmwWJDhZ5spWwSG2ptBf')
consumerSecret=as.character('pwykbVIJYB9eTWNPIlX4Lafb5IFIATJGISX7nrlnof1xEl963x')
access_Token=as.character('1569710470554357760-UeAGnW9nY5VUOQpR38o14Lu0u1m01X')
access_Secret=as.character('Ax0HmTd1bqutfoYcTPJa7X7nE5ZkOI9x5TE9QEPefvM0l')
### Ensure the right URLs are stored to pull data
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter("teen suicide",n=500, since="2020-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))
tweets.corpus <- tweets.corpus %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
as.matrix()
words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(twitteR)
library(ROAuth)
library(jsonlite)
consumerKey=as.character('JKjzpgmwWJDhZ5spWwSG2ptBf')
consumerSecret=as.character('pwykbVIJYB9eTWNPIlX4Lafb5IFIATJGISX7nrlnof1xEl963x')
access_Token=as.character('1569710470554357760-UeAGnW9nY5VUOQpR38o14Lu0u1m01X')
access_Secret=as.character('Ax0HmTd1bqutfoYcTPJa7X7nE5ZkOI9x5TE9QEPefvM0l')
### Ensure the right URLs are stored to pull data
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter("teen suicide",n=500, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))
tweets.corpus <- tweets.corpus %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
as.matrix()
words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter("mental depression",n=500, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))
tweets.corpus <- tweets.corpus %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
as.matrix()
words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter("high school drug",n=500, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))
tweets.corpus <- tweets.corpus %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
as.matrix()
words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(twitteR)
library(ROAuth)
library(jsonlite)
consumerKey=as.character('JKjzpgmwWJDhZ5spWwSG2ptBf')
consumerSecret=as.character('pwykbVIJYB9eTWNPIlX4Lafb5IFIATJGISX7nrlnof1xEl963x')
access_Token=as.character('1569710470554357760-UeAGnW9nY5VUOQpR38o14Lu0u1m01X')
access_Secret=as.character('Ax0HmTd1bqutfoYcTPJa7X7nE5ZkOI9x5TE9QEPefvM0l')
### Ensure the right URLs are stored to pull data
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter("teen suicide",n=500, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))
tweets.corpus <- tweets.corpus %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
as.matrix()
words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search2<-twitteR::searchTwitter("mental depression",n=500, since="2022-01-01")
TweetsDF2<- twListToDF(Search2)
#(TweetsDF$text[1])
TweetsDF2$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF2$text) # remove alphanumeric characters
TweetsDF2$text <- gsub("https\\S*", "",TweetsDF2$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus2 <- Corpus(VectorSource(TweetsDF2$text))
tweets.corpus2 <- tweets.corpus2 %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm2 <- TermDocumentMatrix(tweets.corpus2) %>% # create a term document matrix
as.matrix()
words2 <- sort(rowSums(tdm2), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words2), freq = words2) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud2 <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud2
library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(twitteR)
library(ROAuth)
library(jsonlite)
consumerKey=as.character('JKjzpgmwWJDhZ5spWwSG2ptBf')
consumerSecret=as.character('pwykbVIJYB9eTWNPIlX4Lafb5IFIATJGISX7nrlnof1xEl963x')
access_Token=as.character('1569710470554357760-UeAGnW9nY5VUOQpR38o14Lu0u1m01X')
access_Secret=as.character('Ax0HmTd1bqutfoYcTPJa7X7nE5ZkOI9x5TE9QEPefvM0l')
### Ensure the right URLs are stored to pull data
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter("teen suicide",n=500, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))
tweets.corpus <- tweets.corpus %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
as.matrix()
words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud <- wordcloud2(df,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search2<-twitteR::searchTwitter("mental depression",n=500, since="2022-01-01")
TweetsDF2<- twListToDF(Search2)
#(TweetsDF$text[1])
TweetsDF2$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF2$text) # remove alphanumeric characters
TweetsDF2$text <- gsub("https\\S*", "",TweetsDF2$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus2 <- Corpus(VectorSource(TweetsDF2$text))
tweets.corpus2 <- tweets.corpus2 %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm2 <- TermDocumentMatrix(tweets.corpus2) %>% # create a term document matrix
as.matrix()
words2 <- sort(rowSums(tdm2), decreasing = TRUE) # count all occurrences of each word and group them
df2 <- data.frame(word = names(words2), freq = words2) # convert it to a dataframe
head(df2) # visualize!
set.seed(1234) # for reproducibility
wcloud2 <- wordcloud2(df2,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud2
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search3<-twitteR::searchTwitter("high school drug",n=500, since="2022-01-01")
TweetsDF3<- twListToDF(Search3)
#(TweetsDF$text[1])
TweetsDF3$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF3$text) # remove alphanumeric characters
TweetsDF3$text <- gsub("https\\S*", "",TweetsDF3$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = sT)
# remove all punctuations except # and @.
#create a corpus to allow us clean the text column with tm
tweets.corpus3 <- Corpus(VectorSource(TweetsDF3$text))
tweets.corpus3 <- tweets.corpus3 %>%
tm_map(removeNumbers) %>% # removes numbers from text
tm_map(removePunctuation) %>% # removes punctuation from text
tm_map(stripWhitespace) %>% # trims the text of whitespace
tm_map(content_transformer(tolower)) %>% # convert text to lowercase
tm_map(removeWords,stopwords("english")) %>% # remove stopwords
tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm3 <- TermDocumentMatrix(tweets.corpus3) %>% # create a term document matrix
as.matrix()
words3 <- sort(rowSums(tdm3), decreasing = TRUE) # count all occurrences of each word and group them
df3 <- data.frame(word = names(words3), freq = words3) # convert it to a dataframe
head(df) # visualize!
set.seed(1234) # for reproducibility
wcloud3 <- wordcloud2(df3,   # generate word cloud
size = 1.5,
color= 'random-dark', # set colors
#shape = 'pentagon',
rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud3
